#!/usr/bin/env python3
"""
æ¸¬è©¦çµæ§‹åŒ–ç”ŸæˆåŠŸèƒ½çš„è…³æœ¬
"""

import asyncio
import sys
import os

# æ·»åŠ å¾Œç«¯ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), 'app'))

from app.services.ollama_client import OllamaClient

async def test_structured_generation():
    """æ¸¬è©¦çµæ§‹åŒ–ç”ŸæˆåŠŸèƒ½"""
    
    print("ğŸ§ª æ¸¬è©¦çµæ§‹åŒ–ç”ŸæˆåŠŸèƒ½")
    print("=" * 50)
    
    # å‰µå»º Ollama å®¢æˆ¶ç«¯ - ä½¿ç”¨ Docker ç¶²è·¯åœ°å€
    client = OllamaClient(host="http://ollama:11434", model="llama3")
    
    # æ¸¬è©¦æ¡ˆä¾‹ 1: åŸºæœ¬ç”Ÿæˆ
    print("\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹ 1: åŸºæœ¬è³‡å®‰æŒ‡ä»¤ç”Ÿæˆ")
    print("-" * 30)
    
    result1 = await client.generate_structured_dataset(
        instruction="æ ¹æ“šè³‡é€šå®‰å…¨ç®¡ç†æ³•åˆ†æé€™æ®µç¨‹å¼ç¢¼çš„å®‰å…¨æ€§",
        input_text="""```python
def process_user_data(user_input):
    query = f"INSERT INTO users (data) VALUES ('{user_input}')"
    execute_query(query)
    return "è³‡æ–™å·²å„²å­˜"
```""",
        source=["è³‡é€šå®‰å…¨ç®¡ç†æ³•ç¬¬18æ¢"],
        rejection_reasons=["å…§å®¹éæ–¼ç°¡å–®ï¼Œç¼ºä¹å…·é«”çš„è³‡å®‰é¢¨éšªåˆ†æ", "æ²’æœ‰æä¾›å¯¦ç”¨çš„æ”¹å–„å»ºè­°"]
    )
    
    print("âœ… ç”Ÿæˆçµæœ:")
    print(f"æŒ‡ä»¤: {result1['instruction']}")
    print(f"è¼¸å…¥: {result1['input'][:100]}..." if result1['input'] else "è¼¸å…¥: ç„¡")
    print(f"è¼¸å‡º: {result1['output'][:200]}...")
    
    # æ¸¬è©¦æ¡ˆä¾‹ 2: æ”¿ç­–ç”Ÿæˆ
    print("\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹ 2: è³‡å®‰æ”¿ç­–ç”Ÿæˆ")
    print("-" * 30)
    
    result2 = await client.generate_structured_dataset(
        instruction="ç”Ÿæˆç¬¦åˆå€‹äººè³‡æ–™ä¿è­·æ³•è¦æ±‚çš„éš±ç§æ”¿ç­–",
        system_prompt="ä½ æ˜¯ä¸€ä½è³‡å®‰å°ˆå®¶ï¼Œå°ˆé–€å”åŠ©åˆ¶å®šç¬¦åˆæ³•è¦çš„è³‡å®‰æ”¿ç­–",
        source=["å€‹äººè³‡æ–™ä¿è­·æ³•ç¬¬8æ¢", "å€‹äººè³‡æ–™ä¿è­·æ³•ç¬¬9æ¢"],
        rejection_reasons=["æ”¿ç­–å…§å®¹ä¸å¤ å…·é«”", "ç¼ºä¹å¯¦ä½œæŒ‡å¼•"]
    )
    
    print("âœ… ç”Ÿæˆçµæœ:")
    print(f"æŒ‡ä»¤: {result2['instruction']}")
    print(f"è¼¸å…¥: {result2['input'][:100]}..." if result2['input'] else "è¼¸å…¥: ç„¡")
    print(f"è¼¸å‡º: {result2['output'][:200]}...")
    
    # æ¸¬è©¦æ¡ˆä¾‹ 3: äº‹ä»¶é€šå ±
    print("\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹ 3: è³‡å®‰äº‹ä»¶é€šå ±")
    print("-" * 30)
    
    result3 = await client.generate_structured_dataset(
        instruction="è§£é‡‹è³‡å®‰äº‹ä»¶é€šå ±çš„æ™‚é™è¦æ±‚å’Œç¨‹åº",
        source=["è³‡é€šå®‰å…¨äº‹ä»¶é€šå ±åŠæ‡‰è®Šè¾¦æ³•ç¬¬4æ¢"],
        rejection_reasons=["æ™‚é™èªªæ˜ä¸å¤ æ¸…æ¥š", "ç¼ºå°‘å…·é«”çš„é€šå ±ç¨‹åºæ­¥é©Ÿ"]
    )
    
    print("âœ… ç”Ÿæˆçµæœ:")
    print(f"æŒ‡ä»¤: {result3['instruction']}")
    print(f"è¼¸å…¥: {result3['input'][:100]}..." if result3['input'] else "è¼¸å…¥: ç„¡")
    print(f"è¼¸å‡º: {result3['output'][:200]}...")
    
    print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦å®Œæˆï¼")

async def test_json_parsing():
    """æ¸¬è©¦ JSON è§£æåŠŸèƒ½"""
    
    print("\nğŸ”§ æ¸¬è©¦ JSON è§£æåŠŸèƒ½")
    print("=" * 50)
    
    client = OllamaClient()
    
    # æ¸¬è©¦å„ç¨® JSON æ ¼å¼
    test_responses = [
        # æ¨™æº– JSON
        '{"instruction": "æ¸¬è©¦æŒ‡ä»¤", "input": "æ¸¬è©¦è¼¸å…¥", "output": "æ¸¬è©¦è¼¸å‡º"}',
        
        # å¸¶æœ‰é¡å¤–æ–‡å­—çš„ JSON
        'é€™æ˜¯ä¸€å€‹æ¸¬è©¦å›æ‡‰ã€‚{"instruction": "æ¸¬è©¦æŒ‡ä»¤", "input": "æ¸¬è©¦è¼¸å…¥", "output": "æ¸¬è©¦è¼¸å‡º"} é€™æ˜¯çµå°¾ã€‚',
        
        # å¤šè¡Œ JSON
        '''{
  "instruction": "æ¸¬è©¦æŒ‡ä»¤",
  "input": "æ¸¬è©¦è¼¸å…¥", 
  "output": "æ¸¬è©¦è¼¸å‡º"
}''',
        
        # ç„¡æ•ˆ JSONï¼ˆæ‡‰è©²å›é€€åˆ°åŸå§‹å›æ‡‰ï¼‰
        'é€™ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼'
    ]
    
    for i, response in enumerate(test_responses, 1):
        print(f"\nğŸ“ æ¸¬è©¦æ¡ˆä¾‹ {i}:")
        print(f"åŸå§‹å›æ‡‰: {response[:100]}...")
        
        try:
            # æ¨¡æ“¬è§£æéç¨‹
            response_clean = response.strip()
            start_idx = response_clean.find('{')
            end_idx = response_clean.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response_clean[start_idx:end_idx]
                import json
                parsed = json.loads(json_str)
                print(f"âœ… æˆåŠŸè§£æ JSON: {parsed}")
            else:
                print("âŒ æœªæ‰¾åˆ° JSON ç‰©ä»¶ï¼Œä½¿ç”¨åŸå§‹å›æ‡‰")
        except Exception as e:
            print(f"âŒ JSON è§£æå¤±æ•—: {e}")

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹æ¸¬è©¦çµæ§‹åŒ–ç”ŸæˆåŠŸèƒ½")
    
    # æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦å¯ç”¨
    import httpx
    
    async def check_ollama():
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get("http://ollama:11434/api/version", timeout=5.0)
                if response.status_code == 200:
                    print("âœ… Ollama æœå‹™å¯ç”¨")
                    return True
                else:
                    print("âŒ Ollama æœå‹™å›æ‡‰ç•°å¸¸")
                    return False
        except Exception as e:
            print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™: {e}")
            print("è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œåœ¨ http://ollama:11434")
            return False
    
    async def main():
        if await check_ollama():
            await test_structured_generation()
            await test_json_parsing()
        else:
            print("âŒ è·³éæ¸¬è©¦ï¼Œå› ç‚º Ollama æœå‹™ä¸å¯ç”¨")
    
    asyncio.run(main()) 